{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0MEYJk7dd1Sl"
   },
   "source": [
    "# Partie 3. Classification des textes : analyse de sentiments\n",
    "Dans cette troisième partie du TP, nous allons développer quelques modèles pour l’analyse de sentiments (classement de textes). Le dataset utilisé est un dataset d’analyse de sentiments dans le contexte de la finance (https://www.kaggle.com/datasets/sbhatti/financial-sentiment-analysis). Nous allons utiliser pandas, scikit-learn et nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "iaWP6wCBdh_J"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy  as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import re\n",
    "# On tokenise nous même avec un espace blanc ou un caractère de ponctuation (\\b)\n",
    "token_pattern = re.compile(r'(?u)\\b\\w\\w+\\b')\n",
    "tokenizer = token_pattern.findall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YRE6VeCufOqX"
   },
   "source": [
    "On télécharge le dataset (https://www.kaggle.com/datasets/sbhatti/financial-sentiment-analysis?resource=download) et ajoute son path (ou autre méthode pour charger le fichier avec pandas)\n",
    "\n",
    "## 1. Préparation de données\n",
    "### Question\n",
    "- Importer le dataset CSV fourni en utilisant pandas et afficher les 10 premiers échantillons.\n",
    "\n",
    "Dans mon cas, j'utilise **Colab** je dépose donc mon fichier dans mon drive et renseigne le chemin pour y acceder. Dans votre cas si il tourne en local, renseignez juste le chemin jusqu’au fichier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OlPBG1KMfOWV"
   },
   "outputs": [],
   "source": [
    "# Si vous travaillez sur colab\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iTtL3qZwfYFA"
   },
   "outputs": [],
   "source": [
    "# Le chemin jusqu’à \"data.csv\"\n",
    "path_data = \"/content/drive/MyDrive/PhD/Activities/Enseignements/Fabron DUT2 - Introduction IA/TP5-6/data.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oASBtKarf7gr"
   },
   "source": [
    "Maintenant on importe le csv avec pandas et affiche les 10 premiers échantillons avec la fonction *head()* de pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "jhwUOKlmeduF"
   },
   "outputs": [],
   "source": [
    "# On récupère le csv\n",
    "# TODO\n",
    "# On affiche les 10 premières lignes\n",
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "54N4rVKYgWIb"
   },
   "source": [
    "### Question\n",
    "- Encoder les sentiments en utilisant **LabelBinarizer** pour avoir un encodage OneHot.\n",
    "\n",
    "### Explications\n",
    "LabelBinarizer est une classe du module scikit-learn en Python qui est utilisée pour convertir les étiquettes de classification multi-classes en une représentation binaire.\n",
    "\n",
    "En d'autres termes, si vous avez des données étiquetées avec plusieurs classes, LabelBinarizer convertira chaque étiquette en un vecteur binaire correspondant. Chaque position du vecteur représentera une classe possible et sera soit 1 (si l'exemple appartient à cette classe) ou 0 (si l'exemple n'appartient pas à cette classe).\n",
    "\n",
    "Par exemple, si vous avez un ensemble de données étiquetées avec les classes \"*chat*\", \"*chien*\" et \"*oiseau*\", LabelBinarizer convertira chaque étiquette en un vecteur binaire de longueur 3. Si un exemple est étiqueté comme \"*chien*\", le vecteur correspondant serait [0, 1, 0]. Si un exemple n'appartient à aucune des classes, le vecteur serait [0, 0, 0].\n",
    "\n",
    "La sortie de LabelBinarizer peut être utilisée pour entraîner des modèles de classification binaires tels que les classificateurs de régression logistique ou de SVM, qui ne peuvent gérer que des étiquettes binaires (1 ou 0).\n",
    "\n",
    "Dans notre cas nous souhaitons obtenir un résultat pour les labels \"*positive*\", \"*negative*\", \"*neutral*\" du style:\n",
    "\n",
    "| Sentiment   | OneHot    |\n",
    "|-------------|-----------|\n",
    "| Positive    | [1, 0, 0] |\n",
    "| Neutral     | [0, 1, 0] |\n",
    "| Negative    | [0, 0, 1] |\n",
    "| Autre token | [0, 0, 0] |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "xlNzLkdKfw23"
   },
   "outputs": [],
   "source": [
    "# On crée une instance de LabelBinarizer\n",
    "# TODO\n",
    "\n",
    "# fit_transform() de LabelBinarizer pour encoder les sentiments\n",
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "85lMbWE_qVwE"
   },
   "source": [
    "On regarde ce qu'il se passe et compare avec nos sentiments en texte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "JxvVccC5qCDX"
   },
   "outputs": [],
   "source": [
    "# On remarque que negative -> 0,0, 1; positive -> 1, 0, 0; neutral -> 0, 1, 0;\n",
    "# TODO print"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wvq5F9avjzJW"
   },
   "source": [
    "### Question\n",
    "- Diviser le dataset en entrainement et test, en utilisant train_test_split, la taille du test est 30% et random_state=0.\n",
    "\n",
    "### Aide\n",
    "**train_test_split** est une fonction du module scikit-learn en Python qui permet de diviser un ensemble de données en un ensemble d'entraînement et un ensemble de test. Cette fonction est souvent utilisée dans le cadre de l'apprentissage automatique (machine learning) pour évaluer la performance d'un modèle sur des données indépendantes de celles utilisées pour l'entraînement.\n",
    "\n",
    "La fonction train_test_split prend en entrée les données à diviser ainsi que le pourcentage de données à réserver pour l'ensemble de test. Elle renvoie quatre ensembles de données : *X_train, X_test, y_train, y_test*.\n",
    "\n",
    "Les paramètres les plus couramment utilisés de train_test_split sont :\n",
    "- **test_size** : le pourcentage de données à réserver pour l'ensemble de test. Par exemple, si test_size est fixé à 0.2, cela signifie que 20% des données seront réservées pour le test et que 80% des données seront utilisées pour l'entraînement.\n",
    "- **random_state** : un entier qui permet de fixer la graine aléatoire pour la répartition des données. Cela permet de s'assurer que la même répartition des données sera obtenue à chaque exécution du code, ce qui permet de faciliter la reproductibilité des résultats.\n",
    "\n",
    "### Rappel:\n",
    "Dans le contexte de l'apprentissage automatique supervisé, X représente les variables explicatives (ou caractéristiques) du jeu de données et y représente la variable cible (ou réponse) que nous cherchons à prédire."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "7AL-DR9EhIuD"
   },
   "outputs": [],
   "source": [
    "# Stocker les labels \"Sentence\" dans la variable X\n",
    "# TODO\n",
    "\n",
    "# Diviser l'ensemble de données en ensembles d'entraînement et de test en utilisant la fonction train_test_split de scikit-learn. \n",
    "# L'ensemble de test contiendra 30% des données et la graine aléatoire est fixée à 0 pour assurer la reproductibilité des résultats.\n",
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nN49-t_ClWFd"
   },
   "source": [
    "Si vous êtes curieux ou dubitatif, vérifiez les données dans X et y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "IUO3bV1viERr"
   },
   "outputs": [],
   "source": [
    "# TODO print"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xBGKwNcoljAA"
   },
   "source": [
    "Puis verifiez le ratio 70% train 30% test ainsi que la correspondance de taille X-y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "4cgK9Dd1iHrx"
   },
   "outputs": [],
   "source": [
    "# On check les tailles si on a bien 70/30% \n",
    "# TODO print"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3tQY5b_yl8hl"
   },
   "source": [
    "### Question \n",
    "- Entrainer un modèle de vectorisation TF sur le texte d’entrainement et vectoriser le.\n",
    "\n",
    "### Aide\n",
    "On utilisera **CountVectorizer()**, qui est une fonction de la bibliothèque Scikit-Learn utilisée pour transformer un corpus de documents textuels en une matrice de termes de document, qui représente la fréquence des mots dans chaque document.\n",
    "\n",
    "Plus précisément, CountVectorizer() :\n",
    "\n",
    "- Tokenise les documents : chaque document est divisé en mots ou \"tokens\".\n",
    "- Construit un vocabulaire : les mots uniques de tous les documents sont collectés pour créer un vocabulaire.\n",
    "- Encode chaque document : chaque document est représenté par un vecteur qui compte la fréquence de chaque mot du vocabulaire dans ce document.\n",
    "Le résultat est une matrice creuse (sparse matrix) où chaque ligne correspond à un document et chaque colonne correspond à un mot du vocabulaire. Les valeurs dans la matrice représentent le nombre d'occurrences de chaque mot dans chaque document.\n",
    "\n",
    "Voici un exemple avec les 3 documents suivants:\n",
    "- Le chien rouge joue avec le chat rouge\n",
    "- Le chien est bleu, le chat est rouge, le chien est rouge\n",
    "- Le chat est rouge\n",
    "\n",
    "| Le         | chien | rouge | joue | avec | le | chat | est | bleu |\n",
    "|------------|-------|-------|------|------|----|------|-----|------|\n",
    "| Document 1 | 1     | 1     | 2    | 1    | 1  | 1    | 1   | 0    |\n",
    "| Document 2 | 1     | 2     | 2    | 0    | 0  | 1    | 1   | 2    |\n",
    "| Document 3 | 1     | 0     | 1    | 0    | 0  | 0    | 1   | 1    |\n",
    "\n",
    "Cette matrice peut ensuite être utilisée comme entrée pour des algorithmes de machine learning tels que la régression logistique, les SVM, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "cjTCd0oKqLwN"
   },
   "outputs": [],
   "source": [
    "# Création d'une instance de CountVectorizer\n",
    "# TODO\n",
    "\n",
    "# Utilisation de fit_transform() pour vectoriser le texte d'entraînement\n",
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SjYdkCjrxNsI"
   },
   "source": [
    "Regardons un exemple de plus pres avec le premier document: \"The GeoSolutions technology will leverage Benefon 's GPS solutions by providing Location Based Search Technology , a Communities Platform , location relevant multimedia content and a new and powerful commercial model .\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "4I3t2nt1vPoa"
   },
   "outputs": [],
   "source": [
    "# On prends un exemple\n",
    "# TODO\n",
    "# On affiche la sparse matrice de notre phrase de test\n",
    "# TODO print"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "742lcKsXwBGR"
   },
   "source": [
    "Par exemple, le tuple (0, 1013) 2 signifie que le mot ayant l'indice 1013 dans le vocabulaire du texte d'entraînement (c'est-à-dire le mot correspondant à la 1014ème colonne de la matrice) apparaît deux fois dans la première phrase du texte d'entraînement.\n",
    "\n",
    "Pour afficher les indices de chaque mot dans le vocabulaire correspondant aux colonnes de la matrice, vous pouvez utiliser l'attribut vocabulary_ de l'objet vectorizer, comme ceci :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "fza5CaUfuZT1"
   },
   "outputs": [],
   "source": [
    "# Afficher le dictionnaire\n",
    "# X_voc = # TODO votre vectorizer.vocabulary_\n",
    "# print(X_voc)\n",
    "\n",
    "# # Si vous chercher une valeur en particulier dans le vocabulaire\n",
    "# valeur_recherchee = 1013\n",
    "# for cle, valeur in X_voc.items():\n",
    "#     if valeur == valeur_recherchee:\n",
    "#         print(\"L'index:\", valeur_recherchee, \"correspond au mot:\", cle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PtTOkX7_x_M_"
   },
   "source": [
    "- En utilisant le même modèle, vectoriser le dataset de test.\n",
    "\n",
    "### Aide\n",
    "La méthode fit_transform() de CountVectorizer est une combinaison des méthodes fit() et transform(). Cette méthode est utilisée pour apprendre le vocabulaire du texte d'entraînement et vectoriser le texte en une matrice creuse (sparse matrice). Elle est utilisée pour la première étape de la création d'un modèle de classification de texte, où le vocabulaire est appris à partir du texte d'entraînement et utilisé pour transformer le texte d'entraînement en une représentation vectorielle.\n",
    "\n",
    "La méthode transform() de CountVectorizer, quant à elle, est utilisée pour transformer le texte de test (ou tout autre texte) en une représentation vectorielle en utilisant le vocabulaire appris pendant la phase d'entraînement. Cette méthode est utilisée pour vectoriser le texte de test de la même manière que le texte d'entraînement, en utilisant le même vocabulaire.\n",
    "\n",
    "**En résumé**, fit_transform() est utilisée pour apprendre le vocabulaire et vectoriser le texte d'entraînement, tandis que transform() est utilisée pour vectoriser le texte de test en utilisant le même vocabulaire que celui appris pendant la phase d'entraînement. La méthode fit_transform() doit être utilisée sur le texte d'entraînement, tandis que la méthode transform() doit être utilisée sur le texte de test (ou tout autre texte à vectoriser) après avoir utilisé fit_transform() sur le texte d'entraînement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "SYNWtHx3u9xd"
   },
   "outputs": [],
   "source": [
    "# Utilisation de transform() pour vectoriser le texte de test\n",
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ty-jbEICoeTw"
   },
   "source": [
    "### Question\n",
    "- Définir une fonction tokenstem qui prend un texte et qui utilise tokenizer pour avoir une liste des tokens ensuite elle génère une liste des tokens radicalisés en utilisant nltk.stem.porter.PorterStemmer et elle ne considère pas les tokens appartenant à nltk.corpus.stopwords.words(’english’)\n",
    "\n",
    "\n",
    "On télécharge les données, les stop words en anglais cette fois ci:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FrCSKcc4n7Xq",
    "outputId": "b4556dbd-5274-43d5-86f8-74c7c8ffe957"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "%%capture\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BodZ73SBov5p"
   },
   "source": [
    "Maintenant on crée notre fonction **tokenstem** (tokenisation + remove stopwords + stemming)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "XWiKRoF0kExU"
   },
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def tokenstem(text):\n",
    "    # Tokenisation des mots dans le texte\n",
    "    tokens = tokenizer(text) # On utilise notre fonction de tokenisation défini plus haut\n",
    "    # Création de l'objet PorterStemmer\n",
    "    # Il y a d'autres méthodes que PorterStemmer, vous pouvez comparer\n",
    "    # TODO\n",
    "    # Liste de stopwords en anglais\n",
    "    # TODO\n",
    "    # Racinisation des mots et exclusion des stopwords\n",
    "    # TODO\n",
    "    return stems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9XLgL1MGopgx"
   },
   "source": [
    "On regarde le résultat de note superbe fonction **tokenstem**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "cFxbQ5A4nrHf"
   },
   "outputs": [],
   "source": [
    "# TODO print\n",
    "# print(X_train.loc[0])\n",
    "# print(tokenstem(X_train.loc[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SsWceJfppgo5"
   },
   "source": [
    "### Question\n",
    "-  Refaire les deux étapes précédentes, mais en limitant la taille du vecteur max_features = 3000 et en utilisant tokenstem comme tokenizer.\n",
    "\n",
    "On utilise toujours **CountVectorizer()**, regardez comment l'adapter avec nos rpérequis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "V_RO3YVnnvke"
   },
   "outputs": [],
   "source": [
    "# Création d'une instance de CountVectorizer avec un maximum de 3000 features et en utilisant le tokenizer personnalisé (question précédente)\n",
    "# TODO\n",
    "\n",
    "# Utilisation de fit_transform() pour vectoriser le texte d'entraînement\n",
    "# TODO\n",
    "# Utilisation de transform() pour vectoriser le texte de test\n",
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AT5dwFmxpxp-"
   },
   "source": [
    "## 2. Entrainement\n",
    "### Question\n",
    "- Créer un modèle de type linear support vector machine (SVM)https://scikit-learn.org/stable/modules/svm.html#svm\n",
    "- Prédire les classes en appliquant le modèle sur les représentations d’entrainement\n",
    "- Prédire les classes en appliquant le modèle sur les représentations vectorielles de test\n",
    "\n",
    "### Rappel\n",
    "**SVM**, ou Support Vector Machines, est un algorithme d'apprentissage automatique supervisé utilisé pour la classification et la régression. Il est principalement utilisé pour résoudre des problèmes de classification binaire, où l'objectif est de séparer les données en deux classes distinctes.\n",
    "\n",
    "Le principe de base de SVM consiste à trouver un hyperplan qui sépare les données en deux classes. L'hyperplan est choisi de manière à maximiser la marge entre les deux classes, c'est-à-dire la distance entre les points les plus proches des deux classes. Les points les plus proches des deux classes sont appelés vecteurs de support, d'où le nom de \"Support Vector Machines\".\n",
    "\n",
    "Le type \"*linear*\" de SVM est utilisé lorsque les données peuvent être séparées linéairement, c'est-à-dire lorsque les deux classes peuvent être séparées par un hyperplan. Dans ce cas, l'algorithme de SVM trouve l'hyperplan optimal qui maximise la marge entre les deux classes. L'hyperplan est défini par une fonction linéaire qui prend les caractéristiques d'entrée des données en entrée et produit une sortie qui permet de déterminer la classe à laquelle appartient chaque donnée."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zOXPEmFruRvB"
   },
   "source": [
    "On implemente un SVM avec la première méthode d'entrainement des embeddings.\n",
    "Vous aurez surement besoin d'adapter les données avec la fonction inverse_transform() pour recuperer les labels initiaux."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "CVZZ_ApyptE5"
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "# Définir le modèle SVM\n",
    "# TODO\n",
    "\n",
    "# Entraîner le modèle\n",
    "# TODO\n",
    "\n",
    "# Prédire les classes pour les données de test\n",
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qe6SAyHJuWYb"
   },
   "source": [
    "- Afficher le rapport de classification\n",
    "\n",
    "On utilisera **classification_report**, une fonction de la bibliothèque scikit-learn en Python qui calcule les métriques de performance pour un modèle de classification. Elle prend en entrée les vraies étiquettes de classe (appelées également labels ou ground truth) et les prédictions du modèle, puis calcule un certain nombre de métriques de performance telles que la précision, le rappel, le score F1 et le support.\n",
    "\n",
    "Les métriques de performance calculées par classification_report sont spécifiques à chaque classe. Pour chaque classe, elle calcule les métriques de performance suivantes:\n",
    "\n",
    "- La précision (precision) : la proportion de prédictions positives correctes par rapport à toutes les prédictions positives effectuées par le modèle. Elle mesure la capacité du modèle à identifier correctement les instances de la classe donnée.\n",
    "\n",
    "- Le rappel (recall) : la proportion d'instances positives correctement identifiées par rapport à toutes les instances positives dans les données. Elle mesure la capacité du modèle à identifier toutes les instances positives de la classe donnée.\n",
    "\n",
    "- Le score F1 (F1 score) : une moyenne harmonique de la précision et du rappel qui donne un score unique pour chaque classe. Il est utile lorsque la précision et le rappel ont des valeurs très différentes.\n",
    "\n",
    "- Le support (support) : le nombre d'instances réelles de la classe donnée dans les données.\n",
    "\n",
    "classification_report calcule également ces métriques pour l'ensemble des classes, en prenant en compte la moyenne des métriques sur toutes les classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "Id-NL6d6s_aA"
   },
   "outputs": [],
   "source": [
    "# Évaluer la performance du modèle\n",
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tmyix_9-uZMG"
   },
   "source": [
    "### Question\n",
    "- Créer un modèle de type linear support vector machine (SVM)https://scikit-learn.org/stable/modules/svm.html#svm\n",
    "- Prédire les classes en appliquant le modèle sur les représentations d’entrainement\n",
    "- Prédire les classes en appliquant le modèle sur les représentations vectorielles de test\n",
    "\n",
    "Cette fois ci on experimente avec notre foction tokenstem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "x7NcNk3guNYi"
   },
   "outputs": [],
   "source": [
    "# Définir le modèle SVM\n",
    "# TODO\n",
    "\n",
    "# Entraîner le modèle\n",
    "# TODO\n",
    "\n",
    "# Prédire les classes pour les données de test\n",
    "# TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "bi_TOrifujrl"
   },
   "outputs": [],
   "source": [
    "# Évaluer la performance du modèle\n",
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lnlMinACuNcb"
   },
   "source": [
    "# Fin\n",
    "Libre a vous de continuer et d'experimenter avec d'autresfonctions et paramètres. Vous pouvez par exemple utiliser d'autres méthodes de vectorisation comme: \n",
    "TfidfVectorizer : Cette fonctionnalité est également disponible dans la bibliothèque scikit-learn. Elle utilise le schéma de pondération TF-IDF pour la vectorisation de texte, qui est une technique courante pour représenter les termes dans un document en fonction de leur importance. Elle attribue des poids plus élevés aux termes qui sont fréquents dans un document et rares dans l'ensemble du corpus.\n",
    "\n",
    "- HashingVectorizer : Cette fonctionnalité est également disponible dans la bibliothèque scikit-learn. Elle utilise une fonction de hachage pour convertir les mots en vecteurs de nombres entiers de longueur fixe. Cette fonctionnalité est plus rapide et nécessite moins de mémoire que CountVectorizer car elle ne stocke pas de vocabulaire.\n",
    "\n",
    "- Word2Vec : Cette fonctionnalité utilise un modèle de réseau de neurones pour apprendre des représentations vectorielles denses pour les mots. Elle est capable de capturer des similitudes sémantiques et syntaxiques entre les mots et est utile pour la résolution de tâches telles que la prédiction de mots manquants et la classification de textes.\n",
    "\n",
    "- GloVe : Cette fonctionnalité utilise une approche matricielle pour apprendre des représentations vectorielles pour les mots. Elle est similaire à Word2Vec et est également utile pour la résolution de tâches telles que la classification de textes et la prédiction de mots manquants.\n",
    "\n",
    "- FastText : Cette fonctionnalité est développée par Facebook et utilise une méthode de n-grammes pour apprendre des représentations vectorielles pour les mots. Elle est capable de capturer des similitudes entre les mots qui partagent des sous-chaînes et est utile pour la classification de textes et la prédiction de mots manquants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IBs8hQa-unQk"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}